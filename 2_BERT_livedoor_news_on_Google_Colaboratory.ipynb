{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "2_BERT_livedoor_news_on_Google_Colaboratory.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VYlVm0kpjx57",
        "colab_type": "text"
      },
      "source": [
        "## 日本語BERTでlivedoorニュースを教師あり学習で分類"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IqIW6ar2Bm3J",
        "colab_type": "code",
        "outputId": "44476f53-a1fc-4f50-e953-50441bef9c21",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# 乱数シードの固定\n",
        "\n",
        "import os\n",
        "import random\n",
        "import numpy as np\n",
        "import torch\n",
        "\n",
        "SEED_VALUE = 1234  # これはなんでも良い\n",
        "os.environ['PYTHONHASHSEED'] = str(SEED_VALUE)\n",
        "random.seed(SEED_VALUE)\n",
        "np.random.seed(SEED_VALUE)\n",
        "torch.manual_seed(SEED_VALUE)  # PyTorchを使う場合\n"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x7f280ab59410>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 1
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Ii-mqaAhCApI",
        "colab_type": "text"
      },
      "source": [
        "### GPUの使用可能を確認\n",
        "\n",
        "画面上部のメニュー ランタイム > ランタイムのタイプを変更 で、 ノートブックの設定 を開く\n",
        "\n",
        "ハードウェアアクセラレータに GPU を選択し、 保存 する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "W8TJgawCB_Nb",
        "colab_type": "code",
        "outputId": "81b1d32b-d3f3-494b-f414-dd3d26c9c4b6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# GPUの使用確認：True or False\n",
        "torch.cuda.is_available()\n",
        "\n",
        "# TrueならGPU使用可能"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "gc6dbVUfj1W-",
        "colab_type": "text"
      },
      "source": [
        "## 準備1：Livedoorニュースをダウンロードしてtsvファイル化\n",
        "\n",
        "参考：https://github.com/yoheikikuta/bert-japanese/blob/master/notebook/finetune-to-livedoor-corpus.ipynb\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8Vh0a49Gp-rS",
        "colab_type": "code",
        "outputId": "1b327511-b985-44fa-899f-a23c308ffa6c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 209
        }
      },
      "source": [
        "# Livedoorニュースのファイルをダウンロード\n",
        "! wget \"https://www.rondhuit.com/download/ldcc-20140209.tar.gz\""
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2020-05-19 08:11:22--  https://www.rondhuit.com/download/ldcc-20140209.tar.gz\n",
            "Resolving www.rondhuit.com (www.rondhuit.com)... 59.106.19.174\n",
            "Connecting to www.rondhuit.com (www.rondhuit.com)|59.106.19.174|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 8855190 (8.4M) [application/x-gzip]\n",
            "Saving to: ‘ldcc-20140209.tar.gz.3’\n",
            "\n",
            "ldcc-20140209.tar.g 100%[===================>]   8.44M  1.87MB/s    in 5.2s    \n",
            "\n",
            "2020-05-19 08:11:28 (1.64 MB/s) - ‘ldcc-20140209.tar.gz.3’ saved [8855190/8855190]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "keY2WGdwjzLD",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "e62b13c3-aa74-4b49-cc7b-7cc36cf08e73"
      },
      "source": [
        "# ファイルを解凍し、カテゴリー数と内容を確認\n",
        "import tarfile\n",
        "import os\n",
        "\n",
        "# 解凍\n",
        "tar = tarfile.open(\"ldcc-20140209.tar.gz\", \"r:gz\")\n",
        "tar.extractall(\"./data/livedoor/\")\n",
        "tar.close()\n",
        "\n",
        "# フォルダのファイルとディレクトリを確認\n",
        "files_folders = [name for name in os.listdir(\"./data/livedoor/text/\")]\n",
        "print(files_folders)\n",
        "\n",
        "# カテゴリーのフォルダのみを抽出\n",
        "categories = [name for name in os.listdir(\n",
        "    \"./data/livedoor/text/\") if os.path.isdir(\"./data/livedoor/text/\"+name)]\n",
        "\n",
        "print(\"カテゴリー数:\", len(categories))\n",
        "print(categories)\n"
      ],
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['dokujo-tsushin', 'README.txt', 'it-life-hack', 'CHANGES.txt', 'smax', 'sports-watch', 'kaden-channel', 'movie-enter', 'topic-news', 'livedoor-homme', 'peachy']\n",
            "カテゴリー数: 9\n",
            "['dokujo-tsushin', 'it-life-hack', 'smax', 'sports-watch', 'kaden-channel', 'movie-enter', 'topic-news', 'livedoor-homme', 'peachy']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z201OQ7gvYOY",
        "colab_type": "code",
        "outputId": "9766d63d-029f-4a2c-81b1-accb8ad0dffd",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 176
        }
      },
      "source": [
        "# ファイルの中身を確認してみる\n",
        "file_name = \"./data/livedoor/text/movie-enter/movie-enter-6255260.txt\"\n",
        "\n",
        "with open(file_name) as text_file:\n",
        "    text = text_file.readlines()\n",
        "    print(\"0：\", text[0])  # URL情報\n",
        "    print(\"1：\", text[1])  # タイムスタンプ\n",
        "    print(\"2：\", text[2])  # タイトル\n",
        "    print(\"3：\", text[3])  # 本文\n",
        "\n",
        "    # 今回は4要素目には本文は伸びていないが、4要素目以降に本文がある場合もある\n"
      ],
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "0： http://news.livedoor.com/article/detail/6255260/\n",
            "\n",
            "1： 2012-02-07T09:00:00+0900\n",
            "\n",
            "2： 新しいヴァンパイアが誕生！　ジョニデ主演『ダーク・シャドウ』の公開日が決定\n",
            "\n",
            "3： 　こんなヴァンパイアは見たことがない！　ジョニー・デップとティム・バートン監督がタッグを組んだ映画『ダーク・シャドウズ（原題）』の邦題が『ダーク・シャドウ』に決定。日本公開日が5月19日に決まった。さらに、ジョニー・デップ演じるヴァンパイアの写真が公開された。\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "CoKvaAK1vurV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 本文を取得する前処理関数を定義\n",
        "\n",
        "\n",
        "def extract_main_txt(file_name):\n",
        "    with open(file_name) as text_file:\n",
        "        # 今回はタイトル行は外したいので、3要素目以降の本文のみ使用\n",
        "        text = text_file.readlines()[3:]\n",
        "\n",
        "        # 3要素目以降にも本文が入っている場合があるので、リストにして、後で結合させる\n",
        "        text = [sentence.strip() for sentence in text]  # 空白文字(スペースやタブ、改行)の削除\n",
        "        text = list(filter(lambda line: line != '', text))\n",
        "        text = ''.join(text)\n",
        "        text = text.translate(str.maketrans(\n",
        "            {'\\n': '', '\\t': '', '\\r': '', '\\u3000': ''}))  # 改行やタブ、全角スペースを消す\n",
        "        return text\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vq2ebKoOxThi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# リストに前処理した本文と、カテゴリーのラベルを追加していく\n",
        "import glob\n",
        "\n",
        "list_text = []\n",
        "list_label = []\n",
        "\n",
        "for cat in categories:\n",
        "    text_files = glob.glob(os.path.join(\"./data/livedoor/text\", cat, \"*.txt\"))\n",
        "\n",
        "    # 前処理extract_main_txtを実施して本文を取得\n",
        "    body = [extract_main_txt(text_file) for text_file in text_files]\n",
        "\n",
        "    label = [cat] * len(body)  # bodyの数文だけカテゴリー名のラベルのリストを作成\n",
        "\n",
        "    list_text.extend(body)  # appendが要素を追加するのに対して、extendはリストごと追加する\n",
        "    list_label.extend(label)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bKPb_LJxxuOM",
        "colab_type": "code",
        "outputId": "51a0fb36-5377-4f79-8672-f89f8d88aa51",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# 0番目の文章とラベルを確認\n",
        "print(list_text[0])\n",
        "print(list_label[0])\n"
      ],
      "execution_count": 8,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "婚活中の綾さん(29歳)の理想の結婚相手は転勤のない職種で実家の近くに住んでくれる人。綾さんは二人姉妹の姉。できれば自分の親と同居してくれるマスオさんが理想だそうだ。独女の親との同居率は年々増加している。親と同居していれば、ひとり暮らしの独女より話す頻度も多く繋がりも深くなり、その親密な関係は結婚してもなお続いていくようだ。「毎日実家で夕食を食べています。ママは料理がうまいし、夫もママの手料理を楽しみにしているから、ママも張り切ってます」結婚3年目のマナさん(33歳)は、実家に食費として月に2万円をいれているだけ。家で料理を作らないと光熱費も浮くし、貯金もできる。いずれ実家を二世帯住宅に建て替え、親との同居も考えているそうだ。「休日の度に車で30分の距離のマンションに住む娘が婿と一緒にくる」というのは32歳の既婚の娘を持つ良子さん(57歳)。「家にある食料やもらいものの器や洗剤まで根こそぎ持っていきますね。まるで泥棒が来たみたいです」と苦笑する。「主人は、“お前も嫁に行ったんだから、いつまでも実家を頼ってはいけない”と建て前で叱っていますが、内心は娘の顔が見られるのが嬉しいようです」娘さんは現在妊娠5カ月。出産前後は実家で暮らすそうだ。「今の時代、専業主婦は夢のまた夢。若い世代の収入の減少で、女性は結婚しても働かなければなりません。会社から疲れて帰ってきて、食事の支度をして休日には掃除や洗濯をするなんて、結婚生活は女性の負担が大きいように思います。ましてや女性には出産がありますから、実家の母の手助けは必須だと思いますよ」と良子さん。3歳の子供がいるヒロさん(35歳)も、実家の母がいなければ働きながらの子育ては難しいという。「保育園の緊急連絡先ですが、保育園に一番早く駆けつけることができるのが実家の母なので、1番が実家の母、2番が私。3番目が夫になっています。夫は出張も多くまったく役に立たないので母が頼りです。急なお迎えって滅多にないと思っていたんですけど、元気がなかったり、ちょっとぐったりしていても、すぐに連絡がきます。37度5分でもう緊急連絡です。母が迎えに行って、その足で病院にも連れて行ってくれるので本当に助かります」お子さんが1番好きなのはおばあちゃま。次がママで3番目がパパ。子供の好きな順位が緊急連絡先と同じだとヒロさんは笑うが、幼い子供にとって一番頼りになるのがおばあちゃまなのだろう。息子はお嫁さんの実家にとられてしまってという声もきくが、育児も家事も姑に気がねをしながらお願いするより、実家の母なら頼みやすい。婿と実家の母の関係だが、最近の婿たちは甘え上手。前述の良子さんは、誕生日には婿から花束をもらったそうだ。「メールもしょっちゅうきますし、夫婦喧嘩をするといつも私に電話をかけてきて、仲裁を頼むんですよ。仕事の愚痴も私に聞いてもらうと胸がすっとするといってくれるし」頼られて悪い気はしないと良子さんは嬉しそうだ。これからは嫁の実家が主導で、夫たちは嫁の母にうまく甘えて家庭円満。という時代がくるのかもしれない。とはいえ母のこんな声もある。「仕事をしている娘を応援してやりたい気持ちはありますが、還暦を過ぎると孫の世話は体力的にきついです。娘はもう1人子供を作りたいと、まず私に相談してきましたが、2人目の世話をする自信がないと正直に話しました。夫も孫が赤ん坊の時は抱いたりしていましたけど、今は腰痛がひどくて、赤ん坊を抱く自信がないと言っています」娘の年齢が高くなれば母の年齢も高くなる。結婚後も実家の母に助けてもらいたいなら結婚も出産も急いだ方がよさそうだ。 (オフィスエムツー／佐枝せつこ)\n",
            "dokujo-tsushin\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a_Kf1D9xxuvP",
        "colab_type": "code",
        "outputId": "f70a9ee6-28e7-40c6-8649-4ee921244805",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 214
        }
      },
      "source": [
        "# pandasのDataFrameにする\n",
        "import pandas as pd\n",
        "\n",
        "df = pd.DataFrame({'text': list_text, 'label': list_label})\n",
        "\n",
        "# 大きさを確認しておく（7,376文章が存在）\n",
        "print(df.shape)\n",
        "\n",
        "df.head()\n"
      ],
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(7376, 2)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>婚活中の綾さん(29歳)の理想の結婚相手は転勤のない職種で実家の近くに住んでくれる人。綾さん...</td>\n",
              "      <td>dokujo-tsushin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>佐々木倫子といえば80年代に『動物のお医者さん』を大ヒットさせたベテラン漫画家。獣医を目指す...</td>\n",
              "      <td>dokujo-tsushin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>猛暑続きの今年の夏。7月末、ストッキングの生産数も10年間で4分の1になっているというニュー...</td>\n",
              "      <td>dokujo-tsushin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>監督失格という“タイトルに惹かれて”、マスコミ試写に出かけた。映画『監督失格』は、女優・林由...</td>\n",
              "      <td>dokujo-tsushin</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SNSで学生時代の旧友と再会することも当たり前になってきた。なかには、密かに片思いをしていた...</td>\n",
              "      <td>dokujo-tsushin</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text           label\n",
              "0  婚活中の綾さん(29歳)の理想の結婚相手は転勤のない職種で実家の近くに住んでくれる人。綾さん...  dokujo-tsushin\n",
              "1  佐々木倫子といえば80年代に『動物のお医者さん』を大ヒットさせたベテラン漫画家。獣医を目指す...  dokujo-tsushin\n",
              "2  猛暑続きの今年の夏。7月末、ストッキングの生産数も10年間で4分の1になっているというニュー...  dokujo-tsushin\n",
              "3  監督失格という“タイトルに惹かれて”、マスコミ試写に出かけた。映画『監督失格』は、女優・林由...  dokujo-tsushin\n",
              "4  SNSで学生時代の旧友と再会することも当たり前になってきた。なかには、密かに片思いをしていた...  dokujo-tsushin"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 9
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kB8p83xi02ck",
        "colab_type": "code",
        "outputId": "cdf990df-cf20-474e-8f40-6bdef266d772",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 231
        }
      },
      "source": [
        "# カテゴリーの辞書を作成\n",
        "dic_id2cat = dict(zip(list(range(len(categories))), categories))\n",
        "dic_cat2id = dict(zip(categories, list(range(len(categories)))))\n",
        "\n",
        "print(dic_id2cat)\n",
        "print(dic_cat2id)\n",
        "\n",
        "# DataFrameにカテゴリーindexの列を作成\n",
        "df[\"label_index\"] = df[\"label\"].map(dic_cat2id)\n",
        "df.head()\n",
        "\n",
        "# label列を消去し、text, indexの順番にする\n",
        "df = df.loc[:, [\"text\", \"label_index\"]]\n",
        "df.head()\n"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "{0: 'dokujo-tsushin', 1: 'it-life-hack', 2: 'smax', 3: 'sports-watch', 4: 'kaden-channel', 5: 'movie-enter', 6: 'topic-news', 7: 'livedoor-homme', 8: 'peachy'}\n",
            "{'dokujo-tsushin': 0, 'it-life-hack': 1, 'smax': 2, 'sports-watch': 3, 'kaden-channel': 4, 'movie-enter': 5, 'topic-news': 6, 'livedoor-homme': 7, 'peachy': 8}\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label_index</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>婚活中の綾さん(29歳)の理想の結婚相手は転勤のない職種で実家の近くに住んでくれる人。綾さん...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>佐々木倫子といえば80年代に『動物のお医者さん』を大ヒットさせたベテラン漫画家。獣医を目指す...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>猛暑続きの今年の夏。7月末、ストッキングの生産数も10年間で4分の1になっているというニュー...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>監督失格という“タイトルに惹かれて”、マスコミ試写に出かけた。映画『監督失格』は、女優・林由...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>SNSで学生時代の旧友と再会することも当たり前になってきた。なかには、密かに片思いをしていた...</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label_index\n",
              "0  婚活中の綾さん(29歳)の理想の結婚相手は転勤のない職種で実家の近くに住んでくれる人。綾さん...            0\n",
              "1  佐々木倫子といえば80年代に『動物のお医者さん』を大ヒットさせたベテラン漫画家。獣医を目指す...            0\n",
              "2  猛暑続きの今年の夏。7月末、ストッキングの生産数も10年間で4分の1になっているというニュー...            0\n",
              "3  監督失格という“タイトルに惹かれて”、マスコミ試写に出かけた。映画『監督失格』は、女優・林由...            0\n",
              "4  SNSで学生時代の旧友と再会することも当たり前になってきた。なかには、密かに片思いをしていた...            0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zaE_8vER18xY",
        "colab_type": "code",
        "outputId": "189432c8-db85-488d-a616-b4c00f97eb71",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 196
        }
      },
      "source": [
        "# 順番をシャッフルする\n",
        "df = df.sample(frac=1, random_state=123).reset_index(drop=True)\n",
        "df.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>text</th>\n",
              "      <th>label_index</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>『アヒルと鴨のコインロッカー』（2007年）や『ゴールデンスランバー』（2010年）など、こ...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>「銀のさら」のWeb限定CMが、「差別的な内容だ」と27日からツイッターなどで物議を醸してい...</td>\n",
              "      <td>6</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>TBS「S1」（1日放送分）には、野球クラブチーム・茨城ゴールデンゴールズの片岡安祐美が出演...</td>\n",
              "      <td>3</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>『容疑者Xの献身』『白夜行』『麒麟の翼』など原作、映画ともに大ヒットを記録している国民的作家...</td>\n",
              "      <td>5</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>多くの人と出会うことができるSNS。そのため、恋人作りのために利用している人も多いのではない...</td>\n",
              "      <td>4</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "                                                text  label_index\n",
              "0  『アヒルと鴨のコインロッカー』（2007年）や『ゴールデンスランバー』（2010年）など、こ...            5\n",
              "1  「銀のさら」のWeb限定CMが、「差別的な内容だ」と27日からツイッターなどで物議を醸してい...            6\n",
              "2  TBS「S1」（1日放送分）には、野球クラブチーム・茨城ゴールデンゴールズの片岡安祐美が出演...            3\n",
              "3  『容疑者Xの献身』『白夜行』『麒麟の翼』など原作、映画ともに大ヒットを記録している国民的作家...            5\n",
              "4  多くの人と出会うことができるSNS。そのため、恋人作りのために利用している人も多いのではない...            4"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "TRrt-XH72C3l",
        "colab_type": "code",
        "outputId": "344d24a3-4b2d-4a7b-c9e2-daa538645b55",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "# tsvファイルで保存する\n",
        "\n",
        "# 全体の2割の文章数\n",
        "len_0_2 = len(df) // 5\n",
        "\n",
        "# 前から2割をテストデータとする\n",
        "df[:len_0_2].to_csv(\"./test.tsv\", sep='\\t', index=False, header=None)\n",
        "print(df[:len_0_2].shape)\n",
        "\n",
        "# 前2割からを訓練&検証データとする\n",
        "df[len_0_2:].to_csv(\"./train_eval.tsv\", sep='\\t', index=False, header=None)\n",
        "print(df[len_0_2:].shape)\n"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "(1475, 2)\n",
            "(5901, 2)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "B0NykW7u3Mw9",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# tsvファイルをダウンロードしたい場合\n",
        "from google.colab import files\n",
        "\n",
        "# ダウンロードする場合はコメントを外す\n",
        "# 少し時間がかかる（4MB）\n",
        "# files.download(\"./test.tsv\")\n",
        "\n",
        "\n",
        "# ダウンロードする場合はコメントを外す\n",
        "# 少し時間がかかる（18MB）\n",
        "# files.download(\"./train_eval.tsv\")\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qPT3Pjr94oPW",
        "colab_type": "text"
      },
      "source": [
        "## 準備2：LivedoorニュースをBERT用のDataLoaderにする\n",
        "\n",
        "Hugginfaceのリポジトリの案内とは異なり、torchtextを使用した手法で実装"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kPXX4pr2-kY-",
        "colab_type": "code",
        "outputId": "2a7d359e-0edd-4591-b13a-27e16ce8a075",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 785
        }
      },
      "source": [
        "# MeCabとtransformersの用意\n",
        "!apt install aptitude swig\n",
        "!aptitude install mecab libmecab-dev mecab-ipadic-utf8 git make curl xz-utils file -y\n",
        "!pip install mecab-python3\n",
        "!pip install transformers==2.9.0"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Reading package lists... Done\n",
            "Building dependency tree       \n",
            "Reading state information... Done\n",
            "aptitude is already the newest version (0.8.10-6ubuntu1).\n",
            "swig is already the newest version (3.0.12-1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 31 not upgraded.\n",
            "mecab is already installed at the requested version (0.996-5)\n",
            "libmecab-dev is already installed at the requested version (0.996-5)\n",
            "mecab-ipadic-utf8 is already installed at the requested version (2.7.0-20070801+main-1)\n",
            "git is already installed at the requested version (1:2.17.1-1ubuntu0.7)\n",
            "make is already installed at the requested version (4.1-9.1ubuntu1)\n",
            "curl is already installed at the requested version (7.58.0-2ubuntu3.8)\n",
            "xz-utils is already installed at the requested version (5.2.2-1.3)\n",
            "file is already installed at the requested version (1:5.32-2ubuntu0.4)\n",
            "mecab is already installed at the requested version (0.996-5)\n",
            "libmecab-dev is already installed at the requested version (0.996-5)\n",
            "mecab-ipadic-utf8 is already installed at the requested version (2.7.0-20070801+main-1)\n",
            "git is already installed at the requested version (1:2.17.1-1ubuntu0.7)\n",
            "make is already installed at the requested version (4.1-9.1ubuntu1)\n",
            "curl is already installed at the requested version (7.58.0-2ubuntu3.8)\n",
            "xz-utils is already installed at the requested version (5.2.2-1.3)\n",
            "file is already installed at the requested version (1:5.32-2ubuntu0.4)\n",
            "No packages will be installed, upgraded, or removed.\n",
            "0 packages upgraded, 0 newly installed, 0 to remove and 31 not upgraded.\n",
            "Need to get 0 B of archives. After unpacking 0 B will be used.\n",
            "                            \n",
            "Requirement already satisfied: mecab-python3 in /usr/local/lib/python3.6/dist-packages (0.996.5)\n",
            "Requirement already satisfied: transformers==2.9.0 in /usr/local/lib/python3.6/dist-packages (2.9.0)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (0.1.90)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (2019.12.20)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (1.18.4)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (3.0.12)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (4.41.1)\n",
            "Requirement already satisfied: sacremoses in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (0.0.43)\n",
            "Requirement already satisfied: tokenizers==0.7.0 in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (0.7.0)\n",
            "Requirement already satisfied: dataclasses; python_version < \"3.7\" in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (0.7)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.6/dist-packages (from transformers==2.9.0) (2.23.0)\n",
            "Requirement already satisfied: six in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.9.0) (1.12.0)\n",
            "Requirement already satisfied: click in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.9.0) (7.1.2)\n",
            "Requirement already satisfied: joblib in /usr/local/lib/python3.6/dist-packages (from sacremoses->transformers==2.9.0) (0.14.1)\n",
            "Requirement already satisfied: chardet<4,>=3.0.2 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.0) (3.0.4)\n",
            "Requirement already satisfied: idna<3,>=2.5 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.0) (2.9)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.0) (2020.4.5.1)\n",
            "Requirement already satisfied: urllib3!=1.25.0,!=1.25.1,<1.26,>=1.21.1 in /usr/local/lib/python3.6/dist-packages (from requests->transformers==2.9.0) (1.24.3)\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3tKqo2TF9vzj",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torchtext  # torchtextを使用\n",
        "from transformers.modeling_bert import BertModel\n",
        "from transformers.tokenization_bert_japanese import BertJapaneseTokenizer\n",
        "\n",
        "# 日本語BERTの分かち書き用tokenizerです\n",
        "tokenizer = BertJapaneseTokenizer.from_pretrained(\n",
        "    'bert-base-japanese-whole-word-masking')\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0ZypBWaE-PB6",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# データを読み込んだときに、読み込んだ内容に対して行う処理を定義します\n",
        "\n",
        "max_length = 512  # 東北大学_日本語版の最大の単語数（サブワード数）は512\n",
        "\n",
        "\n",
        "def tokenizer_512(input_text):\n",
        "    \"\"\"torchtextのtokenizerとして扱えるように、512単語のpytorchでのencodeを定義。ここで[0]を指定し忘れないように\"\"\"\n",
        "    return tokenizer.encode(input_text, max_length=512, return_tensors='pt')[0]\n",
        "\n",
        "\n",
        "TEXT = torchtext.data.Field(sequential=True, tokenize=tokenizer_512, use_vocab=False, lower=False,\n",
        "                            include_lengths=True, batch_first=True, fix_length=max_length, pad_token=0)\n",
        "# 注意：tokenize=tokenizer.encodeと、.encodeをつけます。padding[PAD]のindexが0なので、0を指定します。\n",
        "\n",
        "LABEL = torchtext.data.Field(sequential=False, use_vocab=False)\n",
        "\n",
        "# (注釈)：各引数を再確認\n",
        "# sequential: データの長さが可変か？文章は長さがいろいろなのでTrue.ラベルはFalse\n",
        "# tokenize: 文章を読み込んだときに、前処理や単語分割をするための関数を定義\n",
        "# use_vocab：単語をボキャブラリーに追加するかどうか\n",
        "# lower：アルファベットがあったときに小文字に変換するかどうか\n",
        "# include_length: 文章の単語数のデータを保持するか\n",
        "# batch_first：ミニバッチの次元を用意するかどうか\n",
        "# fix_length：全部の文章をfix_lengthと同じ長さになるように、paddingします\n",
        "# init_token, eos_token, pad_token, unk_token：文頭、文末、padding、未知語に対して、どんな単語を与えるかを指定\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "jyLNL-sd_Xd5",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 各tsvファイルを読み込み、分かち書きをしてdatasetにします\n",
        "# 少し時間がかかります\n",
        "# train_eval：5901個、test：1475個\n",
        "dataset_train_eval, dataset_test = torchtext.data.TabularDataset.splits(\n",
        "    path='.', train='train_eval.tsv', test='test.tsv', format='tsv', fields=[('Text', TEXT), ('Label', LABEL)])\n",
        "\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aNRofv_iDOYq",
        "colab_type": "code",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 69
        },
        "outputId": "c4a58711-2136-4466-90b1-76b9241d2d42"
      },
      "source": [
        "# torchtext.data.Datasetのsplit関数で訓練データと検証データを分ける\n",
        "# train_eval：5901個、test：1475個\n",
        "\n",
        "dataset_train, dataset_eval = dataset_train_eval.split(\n",
        "    split_ratio=1.0 - 1475/5901, random_state=random.seed(1234))\n",
        "\n",
        "# datasetの長さを確認してみる\n",
        "print(dataset_train.__len__())\n",
        "print(dataset_eval.__len__())\n",
        "print(dataset_test.__len__())\n"
      ],
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "4426\n",
            "1475\n",
            "1475\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "O_XilciAI2la",
        "colab_type": "code",
        "outputId": "8a3d360a-a6aa-4bae-e653-e2863bf6e690",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 959
        }
      },
      "source": [
        "# datasetの中身を確認してみる\n",
        "item = next(iter(dataset_train))\n",
        "print(item.Text)\n",
        "print(\"長さ：\", len(item.Text))  # 長さを確認 [CLS]から始まり[SEP]で終わる。512より長いと後ろが切れる\n",
        "print(\"ラベル：\", item.Label)\n"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "tensor([    2,  8454,  6172,  1704,  5408,    16,  2109,    20,    16,    33,\n",
            "           29,     6,   802, 28841, 28798,  2935,  8722,    40,   580,    26,\n",
            "           20,    16,    33,  8454,     7,     9,     6,  3799,    18, 20093,\n",
            "           14,    31,  4895,     8,   218,    11,  4021,    16,  6629,     5,\n",
            "           28,     6,  8454,    11,  3062, 20206, 15591,     5,  1628,  6172,\n",
            "          120,     8, 21966,     6,    36,  6740,     5,   745,   536, 11418,\n",
            "           38,    13, 15414,   205,     8,  1216,  8454,  1178,  9494,     5,\n",
            "           74,    35,  2154,  3111,    10,     5,     9,     6,  2154,    94,\n",
            "        13493,   280,     7,    31,    36, 20498,  2154, 25239, 28578,  4815,\n",
            "           38,     8,  1173, 28532, 28450,    91,   587,  8454,  2580,     5,\n",
            "         1075,     9,     6,  2154,    12,  2857,    10,    13,     5,    45,\n",
            "            8,  2381,    11,  5003,     6, 23429,   371,  9542,  1851,     6,\n",
            "         1162,  1134,    29,     8,    26, 28468, 17985,  1851,     5,    51,\n",
            "            7,  1861,    11,  7831,  6715,    13,     6, 12272, 16559,     5,\n",
            "         3208,    14,    73, 27090, 28547,     8,   205, 31958,     6,  1757,\n",
            "           13, 14355,    14,  4611, 28468,    16,  1497,     8,    26, 28456,\n",
            "            6,  8454,     5,  8112,     9,  1499,  2973,    16,    48,   181,\n",
            "            8, 28383,    42, 30348,     5, 12039, 30481,     6, 15437,     6,\n",
            "          893,   326,     8,  5501, 15437,     7,  1610,    62,   434, 16233,\n",
            "          140,  6049,    14,     6,  8454,  5690,     5,  2155, 28614,    13,\n",
            "        12891,    11, 10057,    16,    33,     5,    75,     8,   521,     6,\n",
            "        20498,    12,     9,  1469,     6,  6291,     6,  2672,    64,   324,\n",
            "            5, 22487,    18, 12039, 30481,    11,     6, 15437,     9,  5354,\n",
            "          997,     5, 20915, 28483, 25224,    64,    11,   406,    15,    16,\n",
            "           33,     8,  3188,   326,    13, 12039, 30481,     6,   893, 11853,\n",
            "         1123, 28555,    64,     5,  1367,  8112,    11,  1200,  7294,    12,\n",
            "         9698,  1396,     6, 12039, 30481,     7,  1610,    62,  7249, 30221,\n",
            "        28575,    11,  6824, 28640,    26,   191,    16,     6, 12039, 16503,\n",
            "           11, 18595,     8,  1778,  4422,   312,  1851,   186,     7,     9,\n",
            "         1681,  5558, 28504,    28,  1753, 12798, 28457,     6, 14072, 28470,\n",
            "         9962,    18, 23071,    14, 10385, 28468,    16,    33,     8,   171,\n",
            "            9,  8112,    12,    31,    42, 30348,    11,  9698,  2078,    10,\n",
            "           45,    12,  2824, 12891,    75,    13,    29,     8,    70, 12039,\n",
            "        16503,    11, 20498,     5,  2582,  2442,    12,    28,    31,    36,\n",
            "         4749, 18767, 28477,    38,    13,  2575,     6, 12039, 16503,    11,\n",
            "         8296,  3952,    10,    83,     7, 17213,   326,    11,  8326,    16,\n",
            "         9698,  1396,    16,  3379,    10, 12039, 16503,    11,    36,   287,\n",
            "          246, 18767, 28477,    38,    13,   625,     8,   366,     6, 12039,\n",
            "        16503,     7, 15437,    11,  1334,     6,  7968,    83,     6, 13119,\n",
            "         6641,   118,    13,  8234,    62,     8, 13119,  6641,    12,     9,\n",
            "        12039, 16503,     7, 21859,    11,  1334,    16,     6, 13119,    26,\n",
            "          796,     8,   171,    14,  1113,  1279,    18,  9621,    12,    31,\n",
            "            8, 21859,    14, 12039, 16503,     7,  1610,    62,  6824, 28593,\n",
            "           11,  5170,    15,     6,  7659,    13, 14458,  2988,    11, 18595,\n",
            "            8,  3686,     6, 21859,   230, 13119,    26,   191,    80,    13,\n",
            "            6,  8454,     7,   737,    80,   140,    45,     8, 13119,     9,\n",
            "           57,    32,    12,  9395,     7,  4017,     6,   127,    32,    12,\n",
            "         1122,     6,    36,  1099,  8454,    38,   140,   120,     7,   139,\n",
            "            8,   288,     9, 27335,     8, 25215, 30753, 28965,    64,    12,\n",
            "         1099,  8454,     7,  1610,    62, 21859,    49, 16816,    18,  6865,\n",
            "           11,  8549,     8,   171,   225,  5934,    12,  6297,  7041,  1232,\n",
            "            5,     3])\n",
            "長さ： 512\n",
            "ラベル： 7\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "j7323f5aJFzz",
        "colab_type": "code",
        "outputId": "709bd9d4-b4b5-47c7-d2a6-2b994085a861",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 72
        }
      },
      "source": [
        "# datasetの中身を文章に戻し、確認\n",
        "\n",
        "print(tokenizer.convert_ids_to_tokens(item.Text.tolist()))  # 文章\n",
        "dic_id2cat[int(item.Label)]  # id\n"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "['[CLS]', 'ビール', 'って', 'どう', 'やっ', 'て', '作ら', 'れ', 'て', 'いる', 'か', '、', 'ご', '##存', '##知', '?', '各社', 'から', '発売', 'さ', 'れ', 'て', 'いる', 'ビール', 'に', 'は', '、', 'さまざま', 'な', 'こだわり', 'が', 'ある', '模様', '。', 'それ', 'を', '知っ', 'て', 'おく', 'の', 'も', '、', 'ビール', 'を', '味', '##わう', '楽しみ', 'の', 'ひと', 'って', 'もの', '。', '早速', '、', '「', '大人', 'の', '社会', '科', '見学', '」', 'と', 'いこ', 'う', '。', '国内', 'ビール', '生産', '発祥', 'の', '地', '・', '横浜', '訪れ', 'た', 'の', 'は', '、', '横浜', '市', '鶴見', '区', 'に', 'ある', '「', 'キリン', '横浜', 'ビア', '##ビ', '##レッジ', '」', '。', 'じ', '##つ', '##は', '日本', 'における', 'ビール', '作り', 'の', '歴史', 'は', '、', '横浜', 'で', '始まっ', 'た', 'と', 'の', 'こと', '。', '伝統', 'を', '守る', '、', '由緒', '正', '##しき', '工場', '、', 'といった', 'ところ', 'か', '。', 'さ', '##っ', '##そく', '工場', 'の', '中', 'に', '脚', 'を', '踏み', '##入れる', 'と', '、', 'こんな', '泡', 'の', '演出', 'が', 'お', '出迎', '##え', '。', 'う', '##ぅ', '、', '自然', 'と', '喉', 'が', '鳴', '##っ', 'て', 'しまう', '。', 'さ', '##て', '、', 'ビール', 'の', '原料', 'は', '大きく', '分け', 'て', '3', 'つ', '。', '二条', '大', '##麦', 'の', '麦', '##芽', '、', 'ホップ', '、', 'そして', '水', '。', 'ちなみに', 'ホップ', 'に', '含ま', 'れる', 'ル', '##プリン', 'という', '成分', 'が', '、', 'ビール', '独特', 'の', '苦', '##み', 'と', '香り', 'を', '生み出し', 'て', 'いる', 'の', 'だ', '。', 'なお', '、', 'キリン', 'で', 'は', 'ヨーロッパ', '、', '北米', '、', 'オーストラリア', 'など', '世界', 'の', '良質', 'な', '麦', '##芽', 'を', '、', 'ホップ', 'は', 'チェコ', '産', 'の', 'ファイン', '##ア', '##ロマ', 'など', 'を', '使用', 'し', 'て', 'いる', '。', 'まず', '水', 'と', '麦', '##芽', '、', 'そして', 'コーン', '##スター', '##チ', 'など', 'の', '副', '原料', 'を', '大きな', '釜', 'で', '煮', '##出し', '、', '麦', '##芽', 'に', '含ま', 'れる', 'でん', '##ぷ', '##ん', 'を', '糖', '##化', 'さ', 'せ', 'て', '、', '麦', '汁', 'を', '作り出す', '。', 'そう', 'いえ', 'ば', '工場', '内', 'に', 'は', '香', '##ばし', '##く', 'も', '酸', '##っぱ', '##い', '、', 'なに', '##か', '不思議', 'な', '匂い', 'が', '漂', '##っ', 'て', 'いる', '。', 'これ', 'は', '原料', 'で', 'ある', '大', '##麦', 'を', '煮', '出し', 'た', 'こと', 'で', '生まれる', '香り', 'だ', 'と', 'か', '。', 'この', '麦', '汁', 'を', 'キリン', 'の', 'メイン', '商品', 'で', 'も', 'ある', '「', '一番', '搾', '##り', '」', 'と', 'いい', '、', '麦', '汁', 'を', '一度', 'とっ', 'た', '後', 'に', 'もう一度', '水', 'を', 'いれ', 'て', '煮', '##出し', 'て', '作っ', 'た', '麦', '汁', 'を', '「', '二', '番', '搾', '##り', '」', 'と', 'いう', '。', 'その後', '、', '麦', '汁', 'に', 'ホップ', 'を', '加え', '、', '冷却', '後', '、', '発酵', 'タンク', 'へ', 'と', '移さ', 'れる', '。', '発酵', 'タンク', 'で', 'は', '麦', '汁', 'に', '酵母', 'を', '加え', 'て', '、', '発酵', 'さ', 'せる', '。', 'これ', 'が', '最も', '重要', 'な', '工程', 'で', 'ある', '。', '酵母', 'が', '麦', '汁', 'に', '含ま', 'れる', '糖', '##分', 'を', '分解', 'し', '、', 'アルコール', 'と', '炭酸', 'ガス', 'を', '作り出す', '。', 'つまり', '、', '酵母', 'によって', '発酵', 'さ', 'せ', 'ない', 'と', '、', 'ビール', 'に', 'なら', 'ない', 'という', 'こと', '。', '発酵', 'は', '4', '日', 'で', 'ピーク', 'に', '達し', '、', '7', '日', 'で', '終了', '、', '「', '若', 'ビール', '」', 'という', 'もの', 'に', 'なる', '。', '次', 'は', '濾過', '。', '珪', '##藻', '##土', 'など', 'で', '若', 'ビール', 'に', '含ま', 'れる', '酵母', 'や', '無駄', 'な', 'タンパク質', 'を', '除去', '。', 'これ', 'により', 'クリア', 'で', '美しい', '黄金', '色', 'の', '[SEP]']\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'livedoor-homme'"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4qiBmmdsJ-aK",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# DataLoaderを作成します（torchtextの文脈では単純にiteraterと呼ばれています）\n",
        "batch_size = 16  # BERTでは16、32あたりを使用する\n",
        "\n",
        "dl_train = torchtext.data.Iterator(\n",
        "    dataset_train, batch_size=batch_size, train=True)\n",
        "\n",
        "dl_eval = torchtext.data.Iterator(\n",
        "    dataset_eval, batch_size=batch_size, train=False, sort=False)\n",
        "\n",
        "dl_test = torchtext.data.Iterator(\n",
        "    dataset_test, batch_size=batch_size, train=False, sort=False)\n",
        "\n",
        "# 辞書オブジェクトにまとめる\n",
        "dataloaders_dict = {\"train\": dl_train, \"val\": dl_eval}\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5-sNpiK5K14s",
        "colab_type": "code",
        "outputId": "b2322c1e-977b-4728-82f0-1133716a035c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 121
        }
      },
      "source": [
        "# DataLoaderの動作確認 \n",
        "\n",
        "batch = next(iter(dl_test))\n",
        "print(batch)\n",
        "print(batch.Text[0].shape)\n",
        "print(batch.Label.shape)"
      ],
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "\n",
            "[torchtext.data.batch.Batch of size 16]\n",
            "\t[.Text]:('[torch.LongTensor of size 16x512]', '[torch.LongTensor of size 16]')\n",
            "\t[.Label]:[torch.LongTensor of size 16]\n",
            "torch.Size([16, 512])\n",
            "torch.Size([16])\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v8e6NhQ3cLcq",
        "colab_type": "text"
      },
      "source": [
        "## 準備3：BERTのクラス分類用のモデルを用意する\n",
        "\n",
        "Huggingfaceさんのをそのまま使うのではなく、BERTのbaseだけ使い、残りは自分で実装する"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hFlvnI05a4xN",
        "colab_type": "code",
        "outputId": "0c488a8d-1f1d-45c2-bfc8-f2b40e5e7fa2",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "from transformers.modeling_bert import BertModel\n",
        "\n",
        "# BERTの日本語学習済みパラメータのモデルです\n",
        "model = BertModel.from_pretrained('bert-base-japanese-whole-word-masking')\n",
        "print(model)\n"
      ],
      "execution_count": 23,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "BertModel(\n",
            "  (embeddings): BertEmbeddings(\n",
            "    (word_embeddings): Embedding(32000, 768, padding_idx=0)\n",
            "    (position_embeddings): Embedding(512, 768)\n",
            "    (token_type_embeddings): Embedding(2, 768)\n",
            "    (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "    (dropout): Dropout(p=0.1, inplace=False)\n",
            "  )\n",
            "  (encoder): BertEncoder(\n",
            "    (layer): ModuleList(\n",
            "      (0): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (1): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (2): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (3): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (4): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (5): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (6): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (7): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (8): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (9): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (10): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "      (11): BertLayer(\n",
            "        (attention): BertAttention(\n",
            "          (self): BertSelfAttention(\n",
            "            (query): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (key): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (value): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "          (output): BertSelfOutput(\n",
            "            (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "            (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "            (dropout): Dropout(p=0.1, inplace=False)\n",
            "          )\n",
            "        )\n",
            "        (intermediate): BertIntermediate(\n",
            "          (dense): Linear(in_features=768, out_features=3072, bias=True)\n",
            "        )\n",
            "        (output): BertOutput(\n",
            "          (dense): Linear(in_features=3072, out_features=768, bias=True)\n",
            "          (LayerNorm): LayerNorm((768,), eps=1e-12, elementwise_affine=True)\n",
            "          (dropout): Dropout(p=0.1, inplace=False)\n",
            "        )\n",
            "      )\n",
            "    )\n",
            "  )\n",
            "  (pooler): BertPooler(\n",
            "    (dense): Linear(in_features=768, out_features=768, bias=True)\n",
            "    (activation): Tanh()\n",
            "  )\n",
            ")\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "7BgGd7fLPssV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from torch import nn\n",
        "\n",
        "\n",
        "class BertForLivedoor(nn.Module):\n",
        "    '''BERTモデルにLivedoorニュースの9クラスを判定する部分をつなげたモデル'''\n",
        "\n",
        "    def __init__(self):\n",
        "        super(BertForLivedoor, self).__init__()\n",
        "\n",
        "        # BERTモジュール\n",
        "        self.bert = model  # 日本語学習済みのBERTモデル\n",
        "\n",
        "        # headにポジネガ予測を追加\n",
        "        # 入力はBERTの出力特徴量の次元768、出力は9クラス\n",
        "        self.cls = nn.Linear(in_features=768, out_features=9)\n",
        "\n",
        "        # 重み初期化処理\n",
        "        nn.init.normal_(self.cls.weight, std=0.02)\n",
        "        nn.init.normal_(self.cls.bias, 0)\n",
        "\n",
        "    def forward(self, input_ids):\n",
        "        '''\n",
        "        input_ids： [batch_size, sequence_length]の文章の単語IDの羅列\n",
        "        '''\n",
        "\n",
        "        # BERTの基本モデル部分の順伝搬\n",
        "        # 順伝搬させる\n",
        "        result = self.bert(input_ids)  # reult は、sequence_output, pooled_output\n",
        "\n",
        "        # sequence_outputの先頭の単語ベクトルを抜き出す\n",
        "        vec_0 = result[0]  # 最初の0がsequence_outputを示す\n",
        "        vec_0 = vec_0[:, 0, :]  # 全バッチ。先頭0番目の単語の全768要素\n",
        "        vec_0 = vec_0.view(-1, 768)  # sizeを[batch_size, hidden_size]に変換\n",
        "        output = self.cls(vec_0)  # 全結合層\n",
        "\n",
        "        return output\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IOtveynWRwK5",
        "colab_type": "code",
        "outputId": "e142a63f-2d08-4614-ae02-b9221364c259",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# モデル構築\n",
        "net = BertForLivedoor()\n",
        "\n",
        "# 訓練モードに設定\n",
        "net.train()\n",
        "\n",
        "print('ネットワーク設定完了')\n"
      ],
      "execution_count": 25,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "ネットワーク設定完了\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hkg7r2RIR7qU",
        "colab_type": "text"
      },
      "source": [
        "## 準備4：BERTのファインチューニングの設定"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "wPGYvX4RR3UT",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 勾配計算を最後のBertLayerモジュールと追加した分類アダプターのみ実行\n",
        "\n",
        "# 1. まず全部を、勾配計算Falseにしてしまう\n",
        "for param in net.parameters():\n",
        "    param.requires_grad = False\n",
        "\n",
        "# 2. BertLayerモジュールの最後を勾配計算ありに変更\n",
        "for param in net.bert.encoder.layer[-1].parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "# 3. 識別器を勾配計算ありに変更\n",
        "for param in net.cls.parameters():\n",
        "    param.requires_grad = True\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Il_-wow4Suwe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# 最適化手法の設定\n",
        "import torch.optim as optim\n",
        "\n",
        "\n",
        "# BERTの元の部分はファインチューニング\n",
        "optimizer = optim.Adam([\n",
        "    {'params': net.bert.encoder.layer[-1].parameters(), 'lr': 5e-5},\n",
        "    {'params': net.cls.parameters(), 'lr': 1e-4}\n",
        "])\n",
        "\n",
        "# 損失関数の設定\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "# nn.LogSoftmax()を計算してからnn.NLLLoss(negative log likelihood loss)を計算"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Zu7KRn1bTIQp",
        "colab_type": "text"
      },
      "source": [
        "## 5. 訓練を実施"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZNaAXgiITFiw",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# モデルを学習させる関数を作成\n",
        "\n",
        "\n",
        "def train_model(net, dataloaders_dict, criterion, optimizer, num_epochs):\n",
        "\n",
        "    # GPUが使えるかを確認\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    print(\"使用デバイス：\", device)\n",
        "    print('-----start-------')\n",
        "\n",
        "    # ネットワークをGPUへ\n",
        "    net.to(device)\n",
        "\n",
        "    # ネットワークがある程度固定であれば、高速化させる\n",
        "    torch.backends.cudnn.benchmark = True\n",
        "\n",
        "    # ミニバッチのサイズ\n",
        "    batch_size = dataloaders_dict[\"train\"].batch_size\n",
        "\n",
        "    # epochのループ\n",
        "    for epoch in range(num_epochs):\n",
        "        # epochごとの訓練と検証のループ\n",
        "        for phase in ['train', 'val']:\n",
        "            if phase == 'train':\n",
        "                net.train()  # モデルを訓練モードに\n",
        "            else:\n",
        "                net.eval()   # モデルを検証モードに\n",
        "\n",
        "            epoch_loss = 0.0  # epochの損失和\n",
        "            epoch_corrects = 0  # epochの正解数\n",
        "            iteration = 1\n",
        "\n",
        "            # データローダーからミニバッチを取り出すループ\n",
        "            for batch in (dataloaders_dict[phase]):\n",
        "                # batchはTextとLableの辞書型変数\n",
        "\n",
        "                # GPUが使えるならGPUにデータを送る\n",
        "                inputs = batch.Text[0].to(device)  # 文章\n",
        "                labels = batch.Label.to(device)  # ラベル\n",
        "\n",
        "                # optimizerを初期化\n",
        "                optimizer.zero_grad()\n",
        "\n",
        "                # 順伝搬（forward）計算\n",
        "                with torch.set_grad_enabled(phase == 'train'):\n",
        "\n",
        "                    # BERTに入力\n",
        "                    outputs = net(inputs)\n",
        "\n",
        "                    loss = criterion(outputs, labels)  # 損失を計算\n",
        "\n",
        "                    _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
        "\n",
        "                    # 訓練時はバックプロパゲーション\n",
        "                    if phase == 'train':\n",
        "                        loss.backward()\n",
        "                        optimizer.step()\n",
        "\n",
        "                        if (iteration % 10 == 0):  # 10iterに1度、lossを表示\n",
        "                            acc = (torch.sum(preds == labels.data)\n",
        "                                   ).double()/batch_size\n",
        "                            print('イテレーション {} || Loss: {:.4f} || 10iter. || 本イテレーションの正解率：{}'.format(\n",
        "                                iteration, loss.item(),  acc))\n",
        "\n",
        "                    iteration += 1\n",
        "\n",
        "                    # 損失と正解数の合計を更新\n",
        "                    epoch_loss += loss.item() * batch_size\n",
        "                    epoch_corrects += torch.sum(preds == labels.data)\n",
        "\n",
        "            # epochごとのlossと正解率\n",
        "            epoch_loss = epoch_loss / len(dataloaders_dict[phase].dataset)\n",
        "            epoch_acc = epoch_corrects.double(\n",
        "            ) / len(dataloaders_dict[phase].dataset)\n",
        "\n",
        "            print('Epoch {}/{} | {:^5} |  Loss: {:.4f} Acc: {:.4f}'.format(epoch+1, num_epochs,\n",
        "                                                                           phase, epoch_loss, epoch_acc))\n",
        "\n",
        "    return net\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JfnH-gAmS75e",
        "colab_type": "code",
        "outputId": "e973a53b-f0ff-4b5c-f597-2afbef9f3837",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# 学習・検証を実行する。1epochに2分ほどかかります\n",
        "num_epochs = 4\n",
        "net_trained = train_model(net, dataloaders_dict,\n",
        "                          criterion, optimizer, num_epochs=num_epochs)\n"
      ],
      "execution_count": 29,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "使用デバイス： cuda:0\n",
            "-----start-------\n",
            "イテレーション 10 || Loss: 2.1159 || 10iter. || 本イテレーションの正解率：0.25\n",
            "イテレーション 20 || Loss: 1.9992 || 10iter. || 本イテレーションの正解率：0.375\n",
            "イテレーション 30 || Loss: 1.5570 || 10iter. || 本イテレーションの正解率：0.625\n",
            "イテレーション 40 || Loss: 1.3301 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 50 || Loss: 1.7788 || 10iter. || 本イテレーションの正解率：0.3125\n",
            "イテレーション 60 || Loss: 0.8538 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 70 || Loss: 0.9669 || 10iter. || 本イテレーションの正解率：0.625\n",
            "イテレーション 80 || Loss: 0.8314 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 90 || Loss: 0.4187 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 100 || Loss: 1.3798 || 10iter. || 本イテレーションの正解率：0.625\n",
            "イテレーション 110 || Loss: 0.7275 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 120 || Loss: 0.3552 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 130 || Loss: 0.7527 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 140 || Loss: 0.7155 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 150 || Loss: 0.3241 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 160 || Loss: 0.3351 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 170 || Loss: 0.2045 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 180 || Loss: 0.2370 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 190 || Loss: 0.2049 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 200 || Loss: 0.9191 || 10iter. || 本イテレーションの正解率：0.625\n",
            "イテレーション 210 || Loss: 0.4397 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 220 || Loss: 0.3112 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 230 || Loss: 0.7040 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 240 || Loss: 0.4558 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 250 || Loss: 0.4454 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 260 || Loss: 0.4411 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 270 || Loss: 0.6963 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "Epoch 1/4 | train |  Loss: 0.8414 Acc: 0.7345\n",
            "Epoch 1/4 |  val  |  Loss: 0.3906 Acc: 0.8631\n",
            "イテレーション 10 || Loss: 0.6791 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 20 || Loss: 0.3381 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 30 || Loss: 0.0783 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 40 || Loss: 0.2511 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 50 || Loss: 0.5420 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 60 || Loss: 0.6259 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 70 || Loss: 0.1505 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 80 || Loss: 0.4122 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 90 || Loss: 0.1113 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 100 || Loss: 0.0920 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 110 || Loss: 0.3323 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 120 || Loss: 0.5755 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 130 || Loss: 0.2081 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 140 || Loss: 0.2447 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 150 || Loss: 0.2027 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 160 || Loss: 0.4680 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 170 || Loss: 0.6421 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 180 || Loss: 0.3450 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 190 || Loss: 0.5139 || 10iter. || 本イテレーションの正解率：0.75\n",
            "イテレーション 200 || Loss: 0.4779 || 10iter. || 本イテレーションの正解率：0.6875\n",
            "イテレーション 210 || Loss: 0.1709 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 220 || Loss: 0.0931 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 230 || Loss: 0.0773 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 240 || Loss: 0.1433 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 250 || Loss: 0.2431 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 260 || Loss: 0.1826 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 270 || Loss: 0.1017 || 10iter. || 本イテレーションの正解率：1.0\n",
            "Epoch 2/4 | train |  Loss: 0.3196 Acc: 0.8983\n",
            "Epoch 2/4 |  val  |  Loss: 0.3453 Acc: 0.8902\n",
            "イテレーション 10 || Loss: 0.1392 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 20 || Loss: 0.1767 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 30 || Loss: 0.0272 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 40 || Loss: 0.0962 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 50 || Loss: 0.1329 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 60 || Loss: 0.3433 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 70 || Loss: 0.1884 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 80 || Loss: 0.5107 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 90 || Loss: 0.2070 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 100 || Loss: 0.3270 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 110 || Loss: 0.2003 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 120 || Loss: 0.2357 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 130 || Loss: 0.0670 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 140 || Loss: 0.0734 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 150 || Loss: 0.1997 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 160 || Loss: 0.3603 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 170 || Loss: 0.4984 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 180 || Loss: 0.2214 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 190 || Loss: 0.0837 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 200 || Loss: 0.3777 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 210 || Loss: 0.4441 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 220 || Loss: 0.1876 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 230 || Loss: 0.2209 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 240 || Loss: 0.0388 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 250 || Loss: 0.4409 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 260 || Loss: 0.1074 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 270 || Loss: 0.4357 || 10iter. || 本イテレーションの正解率：0.875\n",
            "Epoch 3/4 | train |  Loss: 0.2253 Acc: 0.9270\n",
            "Epoch 3/4 |  val  |  Loss: 0.2894 Acc: 0.9153\n",
            "イテレーション 10 || Loss: 0.1987 || 10iter. || 本イテレーションの正解率：0.875\n",
            "イテレーション 20 || Loss: 0.0759 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 30 || Loss: 0.0906 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 40 || Loss: 0.1452 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 50 || Loss: 0.1891 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 60 || Loss: 0.0669 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 70 || Loss: 0.2004 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 80 || Loss: 0.0190 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 90 || Loss: 0.0675 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 100 || Loss: 0.2610 || 10iter. || 本イテレーションの正解率：0.8125\n",
            "イテレーション 110 || Loss: 0.1047 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 120 || Loss: 0.0276 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 130 || Loss: 0.0369 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 140 || Loss: 0.0151 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 150 || Loss: 0.0441 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 160 || Loss: 0.0458 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 170 || Loss: 0.0536 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 180 || Loss: 0.2322 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 190 || Loss: 0.1565 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 200 || Loss: 0.1284 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 210 || Loss: 0.2975 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 220 || Loss: 0.0813 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 230 || Loss: 0.1267 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 240 || Loss: 0.0581 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 250 || Loss: 0.0579 || 10iter. || 本イテレーションの正解率：1.0\n",
            "イテレーション 260 || Loss: 0.2493 || 10iter. || 本イテレーションの正解率：0.9375\n",
            "イテレーション 270 || Loss: 0.0209 || 10iter. || 本イテレーションの正解率：1.0\n",
            "Epoch 4/4 | train |  Loss: 0.1562 Acc: 0.9489\n",
            "Epoch 4/4 |  val  |  Loss: 0.2879 Acc: 0.9139\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y8zuZC_0yaOs",
        "colab_type": "text"
      },
      "source": [
        "## テストデータでの性能を確認"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EdxKZzijT5PG",
        "colab_type": "code",
        "outputId": "536411ea-c7e3-48c2-e60f-5c660127aea7",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 52
        }
      },
      "source": [
        "from tqdm import tqdm\n",
        "\n",
        "# テストデータでの正解率を求める\n",
        "device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "\n",
        "net_trained.eval()   # モデルを検証モードに\n",
        "net_trained.to(device)  # GPUが使えるならGPUへ送る\n",
        "\n",
        "# epochの正解数を記録する変数\n",
        "epoch_corrects = 0\n",
        "\n",
        "for batch in tqdm(dl_test):  # testデータのDataLoader\n",
        "    # batchはTextとLableの辞書オブジェクト\n",
        "    # GPUが使えるならGPUにデータを送る\n",
        "    device = torch.device(\"cuda:0\" if torch.cuda.is_available() else \"cpu\")\n",
        "    inputs = batch.Text[0].to(device)  # 文章\n",
        "    labels = batch.Label.to(device)  # ラベル\n",
        "\n",
        "    # 順伝搬（forward）計算\n",
        "    with torch.set_grad_enabled(False):\n",
        "\n",
        "        # BertForLivedoorに入力\n",
        "        outputs = net_trained(inputs)\n",
        "\n",
        "        loss = criterion(outputs, labels)  # 損失を計算\n",
        "        _, preds = torch.max(outputs, 1)  # ラベルを予測\n",
        "        epoch_corrects += torch.sum(preds == labels.data)  # 正解数の合計を更新\n",
        "\n",
        "# 正解率\n",
        "epoch_acc = epoch_corrects.double() / len(dl_test.dataset)\n",
        "\n",
        "print('テストデータ{}個での正解率：{:.4f}'.format(len(dl_test.dataset), epoch_acc))\n"
      ],
      "execution_count": 30,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "100%|██████████| 93/93 [00:24<00:00,  3.79it/s]"
          ],
          "name": "stderr"
        },
        {
          "output_type": "stream",
          "text": [
            "テストデータ1475個での正解率：0.9261\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stderr"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XYvVlrl7y45g",
        "colab_type": "text"
      },
      "source": [
        "https://yoheikikuta.github.io/bert-japanese/\n",
        "\n",
        "https://github.com/yoheikikuta/bert-japanese\n",
        "\n",
        "の「BERT with SentencePiece for Japanese text.」\n",
        "\n",
        "では、入力テキストにタイトルを含めていますが、今回はタイトルは除いています。\n",
        "\n",
        "同様にタイトルを抜いている、[BERTを用いた日本語文書分類タスクの学習・ハイパーパラメータチューニングの実践例](https://medium.com/karakuri/bert%E3%82%92%E7%94%A8%E3%81%84%E3%81%9F%E6%97%A5%E6%9C%AC%E8%AA%9E%E6%96%87%E6%9B%B8%E5%88%86%E9%A1%9E%E3%82%BF%E3%82%B9%E3%82%AF%E3%81%AE%E5%AD%A6%E7%BF%92-%E3%83%8F%E3%82%A4%E3%83%91%E3%83%BC%E3%83%91%E3%83%A9%E3%83%A1%E3%83%BC%E3%82%BF%E3%83%81%E3%83%A5%E3%83%BC%E3%83%8B%E3%83%B3%E3%82%B0%E3%81%AE%E5%AE%9F%E8%B7%B5%E4%BE%8B-2fa5e4299b16)でも、正解率が92%ちょっととなっており、ほぼ同じ正解率が得られました。"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8Y37pHqN2YD0",
        "colab_type": "text"
      },
      "source": [
        "以上。"
      ]
    }
  ]
}